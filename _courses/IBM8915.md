---
title: IBM8915 - Data extraction and preparation
author: Rigel Fernandes
date: 2025-01-16
category: Jekyll
layout: courses
mermaid: true
---

-------------

Gitbook designed to serve as an additional source of information to students of the discipline **IBM8915 – Data Extraction and Preparation**.

Este material complementa aulas, notebooks e atividades avaliativas, servindo como uma referência conceitual e prática sobre os fundamentos de extração, limpeza, transformação e preparação de dados para aplicações em ciência de dados, inteligência artificial e mineração de dados.

# 1. Introdução à Extração e Preparação de Dados
-------------

A extração e preparação de dados são etapas fundamentais no ciclo de ciência de dados, constituindo frequentemente **80% do esforço** total de um projeto.

Dados raramente chegam prontos para análise: podem estar incompletos, despadronizados, ruidosos, inconsistentes, em múltiplos formatos ou distribuídos entre diferentes sistemas.

Nesta disciplina, o objetivo é:
- compreender o ciclo completo de **ETL (Extract, Transform, Load)**;
- analisar dados estruturados e não estruturados;
- conhecer variáveis e suas características estatísticas;
- dominar técnicas de preparação: limpeza, imputação, transformação e redução;
- explorar métodos de agrupamento e seleção de atributos;
- compreender a relação entre mineração de dados e Business Intelligence (BI).

Ao final da disciplina, espera-se que o aluno seja capaz de:
- interpretar adequadamente os tipos de variáveis de um problema;
- identificar problemas estruturais nos dados;
- aplicar técnicas robustas de pré-processamento para melhorar a qualidade das análises;
- entender e implementar o processo KDD (Knowledge Discovery in Databases);
- usar ferramentas modernas de Python para preparar dados reais.

This Colab displays some examples to make it easier to grasp the concepts of this Chapter.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_URL_HERE)

---

# 2. KDD, Mineração de Dados e Relação com BI
-------------

2.1 O processo de KDD

O processo **Knowledge Discovery in Databases (KDD)** descreve as etapas necessárias para descobrir conhecimento útil a partir de grandes volumes de dados.

As etapas são:

1. **Seleção dos dados**
2. **Pré-processamento**
3. **Transformação**
4. **Mineração de dados**
5. **Interpretação e avaliação**

A mineração de dados é apenas uma parte do processo — a mais visível — mas o sucesso depende fortemente da qualidade das etapas anteriores.

2.2 Mineração de Dados (Data Mining)

Mineração de dados é o processo de aplicar algoritmos capazes de identificar padrões, tendências ou agrupamentos automaticamente.  
Técnicas incluem:
- Classificação
- Regressão
- Regras de associação
- Agrupamento
- Seleção de atributos
- Detecção de anomalias

2.3 Relação entre BI e Mineração de Dados

**Business Intelligence (BI)** integra e organiza dados corporativos para permitir decisões operacionais e estratégicas.

Mineração de dados é uma ferramenta avançada dentro do BI, agregando:
- previsão,
- detecção de padrões,
- segmentação,
- análise comportamental.

Enquanto BI responde **“o que aconteceu?”**, mineração de dados responde **“por que aconteceu?”** e **“o que provavelmente vai acontecer?”**.

### Exemplos:
- Segmentação de clientes → agrupamento (clustering)  
- Previsão de vendas → regressão  
- Detecção de fraude → outlier detection  
- Relatórios executivos → OLAP + Data Warehousing

---

# 3. Extração de Dados Estruturados e Não Estruturados
-------------

3.1 Dados estruturados

São organizados em formato tabular: CSV, XLSX, bancos SQL.

Ferramentas comuns:
- Pandas (Python)
- SQL (PostgreSQL, MySQL, SQLite)
- Web APIs (REST)

3.2 Dados semiestruturados

- JSON
- XML
- Logs de sistemas
- Respostas de APIs

3.3 Dados não estruturados

- Texto livre
- PDFs
- Imagens
- Áudio
- Vídeo

### Como transformar dados não estruturados:
- **Tokenização textual**
- **Extração de metadados**
- **Conversão para tabelas**
- **Vetorização de texto (Bag-of-Words, TF-IDF)**

---

# 4. Variáveis: Numéricas, Categóricas e Ordinais
-------------

4.1 Variáveis Numéricas
- **Contínuas**: altura, temperatura, velocidade  
- **Discretas**: quantidade de peças, número de acessos

4.2 Variáveis Categóricas
- **Nominais**: sexo, cor, país  
- **Ordinais**: escolaridade, nível de satisfação, risco de crédito

Variáveis **ordinais** são especialmente importantes porque:
- possuem ordem,
- mas não possuem distância mensurável entre categorias.

Técnicas adequadas:
- Ordinal Encoding  
- Escalas Likert  
- Modelos para dados ordenados

---

# 5. Valores Ausentes (Missing Values)
-------------

Valores ausentes podem surgir por falhas de sensores, formulários incompletos, erros de sistema ou políticas de coleta.

5.1 Tipos de missing

- **MCAR** – Missing Completely at Random  
- **MAR** – Missing at Random  
- **MNAR** – Missing Not at Random  

5.2 Métodos de tratamento
- Remoção completa (listwise deletion)
- Imputação por média/mediana/moda
- KNN Imputer
- MICE (Multiple Imputation by Chained Equations)
- Modelos baseados em regressão

5.3 Avaliação dos impactos
- Viés introduzido
- Mudança na distribuição
- Redução de variância
- Impacto em modelos supervisionados

---

# 6. Outliers na Preparação de Dados
-------------

Outliers são valores que se desviam significativamente do padrão dos dados.

6.1 Técnicas de detecção
- Método do IQR (Interquartile Range)
- Z-score
- Isolation Forest
- DBSCAN
- Local Outlier Factor (LOF)

6.2 Como tratar
- Remoção
- Winsorização
- Transformações (log, Box-Cox)
- Modelos robustos

---

# 7. Análise e Agrupamento de Dados
-------------

O agrupamento (clustering) permite identificar grupos naturais em dados não rotulados.

7.1 Principais métodos
- **K-Means**
- **Hierárquico (Ward, Complete, Single)**
- **DBSCAN**
- **GMM (Gaussian Mixture Models)**

7.2 Etapas
1. Pré-processamento
2. Escolha da métrica de similaridade
3. Definição do número de clusters
4. Execução do algoritmo
5. Validação:
   - Silhouette Score  
   - Davies-Bouldin  
   - Calinski-Harabasz  

7.3 Aplicações práticas
- Segmentação de clientes
- Agrupamento de sensores
- Identificação de tópicos textuais (topic modeling)
- Agrupamento de imagens

---

# 8. Seleção de Atributos para Classificação
-------------

A seleção de atributos melhora desempenho e reduz custo computacional.

8.1 Técnicas de seleção
- **Filter Methods:**
  - Correlação  
  - ANOVA  
  - Qui-quadrado  

- **Wrapper Methods:**
  - Recursive Feature Elimination (RFE)  
  - Forward/Backward Selection  

- **Embedded Methods:**
  - LASSO  
  - Árvores de decisão (importância de features)

8.2 Redução de dimensionalidade
- PCA (Principal Component Analysis)
- LDA (Linear Discriminant Analysis)

8.3 Métricas de utilidade
- Variância explicada
- Informação mútua
- Importância de features (árvores)

---

Suggested Exercises
-------------

**Exercise 1:** Aplique diferentes técnicas de imputação em um dataset real e compare o impacto no desempenho de um classificador.

**Exercise 2:** Identifique e trate outliers em uma base de dados numéricos e discuta o impacto nas distribuições.

**Exercise 3:** Aplique K-Means, DBSCAN e Hierárquico em uma mesma base e compare os resultados com Silhouette Score.

**Exercise 4:** Aplique PCA e RFE para selecionar atributos em um problema de classificação supervisionada.

**Exercise 5:** Classifique variáveis de um dataset entre numéricas, categóricas, nominais e ordinais, explicando sua decisão.

